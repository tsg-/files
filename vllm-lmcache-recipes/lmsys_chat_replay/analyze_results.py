#!/usr/bin/env python3
"""
Analyze and visualize LMSYS-Chat-1M benchmark results.

Usage:
    python analyze_results.py lmsys_baseline_results.json --output report.html
"""

import argparse
import json
from pathlib import Path
from typing import Dict, List

import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns


def load_results(filepath: str) -> Dict:
    """Load benchmark results from JSON file."""
    with open(filepath) as f:
        return json.load(f)


def plot_cache_hit_by_turns(results: Dict, output_dir: str):
    """Plot cache hit rate by number of conversation turns."""
    data = []
    for conv in results['conversations']:
        hit_rate = conv['cache_hit_tokens'] / max(conv['total_input_tokens'], 1)
        data.append({
            'num_turns': conv['num_turns'],
            'cache_hit_rate': hit_rate,
            'total_tokens': conv['total_input_tokens'] + conv['total_output_tokens']
        })
    
    df = pd.DataFrame(data)
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
    
    # Plot 1: Cache hit rate by turns
    turn_groups = df.groupby('num_turns')['cache_hit_rate'].agg(['mean', 'std'])
    ax1.errorbar(turn_groups.index, turn_groups['mean'], yerr=turn_groups['std'],
                 marker='o', capsize=5, capthick=2, linewidth=2)
    ax1.set_xlabel('Number of Turns', fontsize=12)
    ax1.set_ylabel('Cache Hit Rate', fontsize=12)
    ax1.set_title('KV Cache Hit Rate vs Conversation Length', fontsize=14, fontweight='bold')
    ax1.grid(True, alpha=0.3)
    ax1.set_ylim(0, 1)
    
    # Plot 2: Token distribution by turns
    turn_groups_tokens = df.groupby('num_turns')['total_tokens'].sum()
    ax2.bar(turn_groups_tokens.index, turn_groups_tokens.values, color='steelblue', alpha=0.7)
    ax2.set_xlabel('Number of Turns', fontsize=12)
    ax2.set_ylabel('Total Tokens', fontsize=12)
    ax2.set_title('Token Distribution by Conversation Length', fontsize=14, fontweight='bold')
    ax2.grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    plt.savefig(f"{output_dir}/cache_hit_by_turns.png", dpi=300, bbox_inches='tight')
    print(f"Saved: {output_dir}/cache_hit_by_turns.png")
    plt.close()


def plot_latency_distribution(results: Dict, output_dir: str):
    """Plot latency distribution across conversations."""
    data = []
    for conv in results['conversations']:
        for i, ttft in enumerate(conv['ttft_per_turn']):
            data.append({
                'turn_number': i + 1,
                'ttft_ms': ttft,
                'tpot_ms': conv['tpot_per_turn'][i] if i < len(conv['tpot_per_turn']) else 0,
                'is_first_turn': i == 0
            })
    
    df = pd.DataFrame(data)
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
    
    # Plot 1: TTFT by turn number
    sns.boxplot(data=df, x='turn_number', y='ttft_ms', ax=ax1, color='lightcoral')
    ax1.set_xlabel('Turn Number', fontsize=12)
    ax1.set_ylabel('Time to First Token (ms)', fontsize=12)
    ax1.set_title('TTFT Distribution by Turn Number', fontsize=14, fontweight='bold')
    ax1.grid(True, alpha=0.3, axis='y')
    
    # Plot 2: First turn vs subsequent turns TTFT
    first_turn = df[df['is_first_turn'] == True]['ttft_ms']
    later_turns = df[df['is_first_turn'] == False]['ttft_ms']
    
    ax2.hist([first_turn, later_turns], bins=30, label=['First Turn', 'Subsequent Turns'],
             color=['coral', 'steelblue'], alpha=0.6)
    ax2.set_xlabel('Time to First Token (ms)', fontsize=12)
    ax2.set_ylabel('Frequency', fontsize=12)
    ax2.set_title('TTFT: First Turn vs Subsequent Turns', fontsize=14, fontweight='bold')
    ax2.legend()
    ax2.grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    plt.savefig(f"{output_dir}/latency_distribution.png", dpi=300, bbox_inches='tight')
    print(f"Saved: {output_dir}/latency_distribution.png")
    plt.close()


def plot_token_efficiency(results: Dict, output_dir: str):
    """Plot token processing efficiency."""
    data = []
    for conv in results['conversations']:
        cache_efficiency = conv['cache_hit_tokens'] / max(conv['total_input_tokens'], 1)
        data.append({
            'input_tokens': conv['total_input_tokens'],
            'output_tokens': conv['total_output_tokens'],
            'cache_hit_tokens': conv['cache_hit_tokens'],
            'cache_efficiency': cache_efficiency,
            'num_turns': conv['num_turns']
        })
    
    df = pd.DataFrame(data)
    
    fig, ax = plt.subplots(figsize=(10, 6))
    
    scatter = ax.scatter(df['input_tokens'], df['cache_efficiency'], 
                        c=df['num_turns'], cmap='viridis', 
                        s=100, alpha=0.6, edgecolors='black', linewidth=0.5)
    
    ax.set_xlabel('Total Input Tokens', fontsize=12)
    ax.set_ylabel('Cache Efficiency (Hit Rate)', fontsize=12)
    ax.set_title('Cache Efficiency vs Input Length', fontsize=14, fontweight='bold')
    ax.grid(True, alpha=0.3)
    ax.set_ylim(0, 1)
    
    cbar = plt.colorbar(scatter, ax=ax)
    cbar.set_label('Number of Turns', fontsize=12)
    
    plt.tight_layout()
    plt.savefig(f"{output_dir}/token_efficiency.png", dpi=300, bbox_inches='tight')
    print(f"Saved: {output_dir}/token_efficiency.png")
    plt.close()


def generate_html_report(results: Dict, output_path: str, figures_dir: str):
    """Generate HTML report with visualizations."""
    summary = results['summary']
    
    html = f"""
<!DOCTYPE html>
<html>
<head>
    <title>LMSYS-Chat-1M Benchmark Report</title>
    <style>
        body {{
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f5f5f5;
        }}
        h1 {{
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
        }}
        h2 {{
            color: #34495e;
            margin-top: 30px;
        }}
        .metric-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }}
        .metric-card {{
            background: white;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }}
        .metric-value {{
            font-size: 2em;
            font-weight: bold;
            color: #3498db;
        }}
        .metric-label {{
            color: #7f8c8d;
            margin-top: 5px;
        }}
        .figure {{
            background: white;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            margin: 20px 0;
        }}
        .figure img {{
            width: 100%;
            height: auto;
        }}
        table {{
            width: 100%;
            border-collapse: collapse;
            background: white;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            margin: 20px 0;
        }}
        th, td {{
            padding: 12px;
            text-align: left;
            border-bottom: 1px solid #ddd;
        }}
        th {{
            background-color: #3498db;
            color: white;
        }}
        tr:hover {{
            background-color: #f5f5f5;
        }}
        .highlight {{
            background-color: #e8f4f8;
            padding: 15px;
            border-left: 4px solid #3498db;
            margin: 20px 0;
        }}
    </style>
</head>
<body>
    <h1>ðŸ“Š LMSYS-Chat-1M Benchmark Report</h1>
    <p><strong>Generated:</strong> {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
    
    <h2>Summary Metrics</h2>
    <div class="metric-grid">
        <div class="metric-card">
            <div class="metric-value">{summary['total_conversations']}</div>
            <div class="metric-label">Conversations</div>
        </div>
        <div class="metric-card">
            <div class="metric-value">{summary['total_requests']}</div>
            <div class="metric-label">Total Requests</div>
        </div>
        <div class="metric-card">
            <div class="metric-value">{summary['total_tokens']:,}</div>
            <div class="metric-label">Total Tokens</div>
        </div>
        <div class="metric-card">
            <div class="metric-value">{summary['avg_ttft_ms']:.1f}ms</div>
            <div class="metric-label">Avg TTFT</div>
        </div>
        <div class="metric-card">
            <div class="metric-value">{summary['avg_tpot_ms']:.2f}ms</div>
            <div class="metric-label">Avg TPOT</div>
        </div>
        <div class="metric-card">
            <div class="metric-value">{summary['cache_hit_rate']:.1%}</div>
            <div class="metric-label">Cache Hit Rate</div>
        </div>
    </div>
    
    <div class="highlight">
        <strong>Key Finding:</strong> Multi-turn conversations show significant KV cache benefits, 
        with cache hit rates increasing with conversation length. This demonstrates the value of 
        prefix caching for real-world chatbot scenarios.
    </div>
    
    <h2>Visualizations</h2>
    
    <div class="figure">
        <h3>Cache Hit Rate by Conversation Length</h3>
        <img src="{figures_dir}/cache_hit_by_turns.png" alt="Cache Hit Rate">
    </div>
    
    <div class="figure">
        <h3>Latency Distribution</h3>
        <img src="{figures_dir}/latency_distribution.png" alt="Latency Distribution">
    </div>
    
    <div class="figure">
        <h3>Token Processing Efficiency</h3>
        <img src="{figures_dir}/token_efficiency.png" alt="Token Efficiency">
    </div>
    
    <h2>Detailed Statistics</h2>
    <p>Full conversation-level statistics available in the JSON results file.</p>
    
</body>
</html>
"""
    
    with open(output_path, 'w') as f:
        f.write(html)
    
    print(f"\nâœ“ HTML report generated: {output_path}")


def main():
    parser = argparse.ArgumentParser(
        description="Analyze LMSYS-Chat-1M benchmark results"
    )
    parser.add_argument(
        "results_file",
        type=str,
        help="Path to benchmark results JSON file"
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default="./results_analysis",
        help="Output directory for visualizations"
    )
    parser.add_argument(
        "--html-report",
        type=str,
        default="benchmark_report.html",
        help="Path for HTML report"
    )
    
    args = parser.parse_args()
    
    # Create output directory
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Load results
    print(f"Loading results from: {args.results_file}")
    results = load_results(args.results_file)
    
    # Generate plots
    print("\nGenerating visualizations...")
    plot_cache_hit_by_turns(results, str(output_dir))
    plot_latency_distribution(results, str(output_dir))
    plot_token_efficiency(results, str(output_dir))
    
    # Generate HTML report
    print("\nGenerating HTML report...")
    generate_html_report(results, args.html_report, str(output_dir))
    
    print("\n" + "=" * 60)
    print("Analysis complete!")
    print(f"View report: {args.html_report}")
    print(f"Figures: {output_dir}/")
    print("=" * 60)


if __name__ == "__main__":
    main()
