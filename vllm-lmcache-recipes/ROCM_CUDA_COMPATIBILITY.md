# ROCm CUDA API Compatibility - Critical Information

**Date**: October 19, 2025  
**Topic**: Why LMCache works on MI300X without modification

---

## ğŸ¯ TL;DR

**LMCache's CUDA code runs on AMD MI300X GPUs without modification because PyTorch ROCm deliberately reuses the `torch.cuda.*` API.**

---

## â“ The Question

If LMCache uses CUDA-specific code like:
```python
torch.cuda.device_count()
torch.cuda.set_device(local_rank)
device = torch.device(f"cuda:{local_rank}")
slot_mapping = slot_mapping.cuda()
```

How can it work on AMD MI300X GPUs?

---

## âœ… The Answer

### From Official PyTorch Documentation

> **"HIP Interfaces Reuse the CUDA Interfaces"**
>
> PyTorch for HIP intentionally reuses the existing `torch.cuda` interfaces. This helps to accelerate the porting of existing PyTorch code and models because **very few code changes are necessary, if any**.

**Source**: [PyTorch HIP (ROCm) Semantics](https://pytorch.org/docs/stable/notes/hip.html)

### What This Means

1. **`torch.cuda` is the correct API for both NVIDIA and AMD GPUs**
   - NOT `torch.rocm` or `torch.hip`
   - NOT `torch.amd`

2. **Code written for CUDA works on ROCm automatically**
   ```python
   # This works on BOTH NVIDIA and AMD GPUs
   device = torch.device('cuda:0')
   tensor = torch.randn(100).cuda()
   ```

3. **Device detection works the same**
   ```python
   # On MI300X system with ROCm installed:
   torch.cuda.is_available()  # Returns True
   torch.cuda.device_count()  # Returns number of AMD GPUs
   ```

---

## ğŸ” Deep Dive: How ROCm Works

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           PyTorch Python API Layer              â”‚
â”‚                                                 â”‚
â”‚         torch.cuda.* (same for all)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â†“                                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  NVIDIA Backend   â”‚        â”‚    AMD Backend       â”‚
â”‚                   â”‚        â”‚                      â”‚
â”‚  CUDA Runtime     â”‚        â”‚   HIP Runtime        â”‚
â”‚  cuBLAS           â”‚        â”‚   rocBLAS            â”‚
â”‚  cuDNN            â”‚        â”‚   MIOpen             â”‚
â”‚  NCCL             â”‚        â”‚   RCCL               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### HIP Translation Layer

**HIP (Heterogeneous-compute Interface for Portability)** is AMD's C++ dialect that:
- Translates CUDA API calls to ROCm equivalents
- Maintains source-level compatibility
- Runs the same Python code on different hardware

### API Mapping Examples

| Python Code | NVIDIA | AMD MI300X |
|-------------|--------|------------|
| `torch.cuda.is_available()` | Checks CUDA | Checks ROCm |
| `torch.device('cuda:0')` | GPU 0 (NVIDIA) | GPU 0 (AMD) |
| `.cuda()` | Copy to NVIDIA GPU | Copy to AMD GPU |
| `torch.cuda.synchronize()` | `cudaDeviceSynchronize()` | `hipDeviceSynchronize()` |

---

## ğŸ“Š Verification Example

### On System Without GPUs (CPU-only)
```python
import torch
print(torch.cuda.is_available())  # False - no GPUs present
```

### On NVIDIA System with CUDA
```python
import torch
print(torch.cuda.is_available())      # True
print(torch.version.cuda)             # "12.1" (CUDA version)
print(torch.cuda.get_device_name(0))  # "NVIDIA H100 80GB HBM3"
```

### On AMD System with ROCm
```python
import torch
print(torch.cuda.is_available())      # True (uses ROCm!)
print(torch.version.hip)              # "6.0.32830" (HIP version)
print(torch.cuda.get_device_name(0))  # "AMD Instinct MI300X"
```

**Key Point**: Notice that `torch.cuda.is_available()` returns `True` on AMD systems!

---

## ğŸ“ Why This Design?

### Benefits of CUDA API Reuse

1. **Code Portability**: Same PyTorch code runs on NVIDIA and AMD
2. **Ecosystem Compatibility**: Libraries written for CUDA work on ROCm
3. **Developer Experience**: No need to learn new APIs
4. **Model Compatibility**: Pretrained models work across vendors

### AMD's Strategy

AMD deliberately chose to maintain CUDA API compatibility to:
- Lower barrier to adoption
- Leverage existing CUDA ecosystem
- Enable drop-in replacement for NVIDIA GPUs

---

## ğŸ”§ Implications for LMCache

### Original Code (CUDA-based)
```python
# From lmcache/integration/vllm/vllm_v1_adapter.py
num_gpus = torch.cuda.device_count()
local_rank = parallel_config.rank % num_gpus
torch.cuda.set_device(local_rank)
device = torch.device(f"cuda:{local_rank}")
```

### On MI300X System
This **exact same code** works because:
- `torch.cuda.device_count()` â†’ Returns number of AMD GPUs
- `torch.cuda.set_device()` â†’ Sets active AMD GPU
- `torch.device("cuda:0")` â†’ References AMD GPU 0

### No Modification Needed! âœ…

---

## ğŸš¨ When DO You Need Different Code?

### Cases Requiring Platform-Specific Code

Only when using **vendor-specific features**:

```python
# Example: Checking which backend
if torch.version.cuda:
    # NVIDIA-specific optimization
    use_flash_attention_2()
elif torch.version.hip:
    # AMD-specific optimization
    use_composable_kernel()
```

### LMCache Doesn't Need This

LMCache uses only **standard PyTorch operations** that work on both:
- Tensor operations (`.cuda()`, `.to()`)
- Device management (`torch.cuda.set_device()`)
- Memory management (`torch.cuda.synchronize()`)

All of these are **vendor-agnostic** through PyTorch's abstraction layer.

---

## ğŸ“ Documentation Accuracy Review

### Claims in Our Documentation

| Claim | Status | Evidence |
|-------|--------|----------|
| "LMCache works on MI300X" | âœ… Correct | ROCm reuses torch.cuda API |
| "MI300X uses CUDA code" | âœ… Correct | Same Python API, different backend |
| "No modification needed" | âœ… Correct | PyTorch handles translation |
| "Storage bridges CUDA/HPU" | âœ… Correct | Storage is device-agnostic |
| "Heterogeneous arch works" | âœ… Correct | Storage connects any backends |

### What About Gaudi3?

**Gaudi3 DOES need modification** because:
- Intel uses `habana_frameworks` backend
- Different API: `torch.hpu` instead of `torch.cuda`
- Not designed for CUDA compatibility
- Requires explicit porting (our patch!)

---

## ğŸ¯ Summary

### Three-Platform Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   LMCache + vLLM                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â†“                â†“                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  NVIDIA GPU   â”‚ â”‚   AMD MI300X â”‚ â”‚  Intel Gaudi3  â”‚
â”‚               â”‚ â”‚              â”‚ â”‚                â”‚
â”‚  torch.cuda   â”‚ â”‚  torch.cuda  â”‚ â”‚  torch.hpu     â”‚
â”‚  (native)     â”‚ â”‚  (via ROCm)  â”‚ â”‚  (patched)     â”‚
â”‚               â”‚ â”‚              â”‚ â”‚                â”‚
â”‚  âœ… Works     â”‚ â”‚  âœ… Works    â”‚ â”‚  âœ… Works      â”‚
â”‚  (original)   â”‚ â”‚  (original)  â”‚ â”‚  (with patch)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Takeaways

1. **MI300X does NOT need a patch** - ROCm maintains CUDA API compatibility
2. **Gaudi3 DOES need a patch** - Intel uses different API (`torch.hpu`)
3. **Our documentation is accurate** - All claims about MI300X are correct
4. **Heterogeneous architecture works** - Storage bridges all platforms

---

## ğŸ”¬ Testing Recommendations

### To Verify on MI300X System

```bash
# 1. Check ROCm installation
rocm-smi

# 2. Verify PyTorch sees AMD GPUs
python3 -c "import torch; print('GPUs:', torch.cuda.device_count())"
# Expected: GPUs: 8 (for 8x MI300X)

# 3. Check HIP version
python3 -c "import torch; print('HIP:', torch.version.hip)"
# Expected: HIP: 6.0.32830 (or similar)

# 4. Run LMCache import test
python3 -c "from lmcache.integration.vllm.vllm_v1_adapter import LMCacheConnectorV1Impl"
# Expected: No errors (works without patch!)
```

### To Verify on Gaudi3 System

```bash
# 1. Check Gaudi devices
hl-smi

# 2. Verify PyTorch sees HPU
python3 -c "import habana_frameworks.torch.core as htcore; import torch; print('HPU:', torch.hpu.is_available())"
# Expected: HPU: True

# 3. Apply patch first!
cd LMCache
git apply vllm_v1_adapter_gaudi3.patch

# 4. Run LMCache import test
python3 -c "from lmcache.integration.vllm.vllm_v1_adapter import LMCacheConnectorV1Impl"
# Expected: No errors (works with patch!)
```

---

## ğŸ“š References

1. **PyTorch HIP Semantics**: https://pytorch.org/docs/stable/notes/hip.html
2. **ROCm Documentation**: https://rocm.docs.amd.com/
3. **HIP Programming Guide**: https://rocm.docs.amd.com/projects/HIP/
4. **AMD MI300X Specs**: https://www.amd.com/en/products/accelerators/instinct/mi300/mi300x.html

---

## ğŸ¤” FAQ

**Q: Why does my laptop show `torch.cuda.is_available() = False`?**  
A: Your laptop doesn't have CUDA or ROCm installed. This is expected on CPU-only systems.

**Q: Will LMCache work on other AMD GPUs like MI250X?**  
A: Yes! Any AMD GPU with ROCm support will work.

**Q: Do I need to install CUDA drivers for MI300X?**  
A: No! Install ROCm drivers instead. PyTorch ROCm builds use the `torch.cuda` API but talk to ROCm backend.

**Q: What about Intel Arc GPUs?**  
A: Intel Arc uses different backend (Intel Extension for PyTorch). Would need separate porting effort.

**Q: Can I mix NVIDIA and AMD GPUs in the same system?**  
A: Technically possible but complex - requires careful device management. Our disaggregated architecture (separate clusters) is cleaner.

---

**Last Updated**: October 19, 2025  
**Status**: âœ… Verified with official PyTorch documentation
