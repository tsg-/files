From 920b325367bd0a594f4983adc8fa80b64d09e7ec Mon Sep 17 00:00:00 2001
From: Tushar Gohad <tgohad@Tushars-MacBook-Pro.local>
Date: Sun, 19 Oct 2025 08:57:05 -0700
Subject: [PATCH] Add Intel Gaudi3 HPU support to vLLM v1 adapter

- Add HPU device detection alongside CUDA
- Replace .cuda() calls with device-agnostic .to() calls
- Add htcore.mark_step() synchronization for HPU
- Maintain backward compatibility with CUDA
- Graceful fallback when HPU not available
---
 lmcache/integration/vllm/vllm_v1_adapter.py | 99 +++++++++++++++++++--
 1 file changed, 92 insertions(+), 7 deletions(-)

diff --git a/lmcache/integration/vllm/vllm_v1_adapter.py b/lmcache/integration/vllm/vllm_v1_adapter.py
index 6301b01..82d06c8 100644
--- a/lmcache/integration/vllm/vllm_v1_adapter.py
+++ b/lmcache/integration/vllm/vllm_v1_adapter.py
@@ -23,6 +23,12 @@ from vllm.v1.core.sched.output import SchedulerOutput
 from vllm.version import __version__ as VLLM_VERSION
 import torch
 
+# HPU-specific imports for Intel Gaudi3
+try:
+    import habana_frameworks.torch.core as htcore
+except ImportError:
+    htcore = None
+
 # First Party
 from lmcache import utils
 from lmcache.config import LMCacheEngineMetadata
@@ -474,10 +480,21 @@ def _init_lmcache_engine(
     )
 
     # Change current device.
-    num_gpus = torch.cuda.device_count()
-    local_rank = parallel_config.rank % num_gpus
-    torch.cuda.set_device(local_rank)
-    device = torch.device(f"cuda:{local_rank}")
+    # Detect device type: CUDA or HPU
+    if torch.cuda.is_available():
+        num_devices = torch.cuda.device_count()
+        local_rank = parallel_config.rank % num_devices
+        torch.cuda.set_device(local_rank)
+        device = torch.device(f"cuda:{local_rank}")
+        device_type = "cuda"
+    elif torch.hpu.is_available() and htcore is not None:
+        num_devices = torch.hpu.device_count()
+        local_rank = parallel_config.rank % num_devices
+        torch.hpu.set_device(local_rank)
+        device = torch.device(f"hpu:{local_rank}")
+        device_type = "hpu"
+    else:
+        raise RuntimeError("Neither CUDA nor HPU devices available")
     metadata = LMCacheEngineMetadata(
         model_config.model,
         parallel_config.world_size,
@@ -690,9 +707,21 @@ class LMCacheConnectorV1Impl:
         else:
             self.api_server = None  # type: ignore[assignment]
             self.plugin_launcher = None  # type: ignore[assignment]
+        # Store device type for later use
+        if hasattr(self, 'lmcache_engine') and self.lmcache_engine is not None:
+            # Infer device type from engine's device
+            engine_device = getattr(self.lmcache_engine.gpu_connector, 'device', None)
+            if engine_device is not None:
+                self.device_type = engine_device.type
+            else:
+                self.device_type = "cuda" if torch.cuda.is_available() else "cpu"
+        else:
+            self.device_type = None
+            
         logger.info(
             f"LMCache initialized for role {role} with version {utils.get_version()}, "
             f"vllm version {VLLM_VERSION}, "
+            f"device type: {self.device_type}, "
             "lmcache cache_engine metadata: "
             f"{getattr(self.lmcache_engine, 'metadata', None)}"
         )
@@ -819,7 +848,23 @@ class LMCacheConnectorV1Impl:
 
             tokens = request.token_ids
             # TODO: have a pre-allocated buffer to hold the slot_mappings
-            slot_mapping = request.slot_mapping.cuda()
+            # Move to appropriate device (CUDA or HPU)
+
+            if hasattr(self, 'device_type') and self.device_type == "hpu":
+
+                slot_mapping = request.slot_mapping.to('hpu', non_blocking=True)
+
+            elif hasattr(self, 'device_type') and self.device_type == "cuda":
+
+                slot_mapping = request.slot_mapping.cuda()
+
+            else:
+
+                # Fallback: try to infer from engine device
+
+                slot_mapping = request.slot_mapping.to(
+
+                    self.lmcache_engine.gpu_connector.device, non_blocking=True)
             assert len(tokens) == len(slot_mapping)
 
             self._stats_monitor.update_interval_vllm_hit_tokens(
@@ -899,6 +944,10 @@ class LMCacheConnectorV1Impl:
         if self.layerwise_retrievers:
             logger.debug(f"Waiting for layer {self.current_layer} to be loaded")
 
+        # HPU synchronization
+        if hasattr(self, 'device_type') and self.device_type == "hpu" and htcore is not None:
+            htcore.mark_step()
+
         # Wait for the layer to be loaded
         for layerwise_retriever in self.layerwise_retrievers:
             ret_token_mask = next(layerwise_retriever)
@@ -965,7 +1014,21 @@ class LMCacheConnectorV1Impl:
                 assert len(slot_mapping) == len(token_ids)
 
                 # TODO: have a pre-allocated buffer to hold the slot_mappings
-                slot_mapping = slot_mapping.cuda()
+                # Move to appropriate device
+
+                if hasattr(self, 'device_type') and self.device_type == "hpu":
+
+                    slot_mapping = slot_mapping.to('hpu', non_blocking=True)
+
+                elif hasattr(self, 'device_type') and self.device_type == "cuda":
+
+                    slot_mapping = slot_mapping.cuda()
+
+                else:
+
+                    slot_mapping = slot_mapping.to(
+
+                        self.lmcache_engine.gpu_connector.device, non_blocking=True)
 
                 if self.kv_role == "kv_producer":
                     skip_leading_tokens = 0
@@ -1015,6 +1078,10 @@ class LMCacheConnectorV1Impl:
     @_lmcache_nvtx_annotate
     def wait_for_save(self):
         """Blocking until the KV cache is saved to the connector buffer."""
+        
+        # HPU synchronization before checking save status
+        if hasattr(self, 'device_type') and self.device_type == "hpu" and htcore is not None:
+            htcore.mark_step()
 
         connector_metadata = self._parent._get_connector_metadata()
         assert isinstance(connector_metadata, LMCacheConnectorMetadata)
@@ -1054,7 +1121,21 @@ class LMCacheConnectorV1Impl:
             assert len(slot_mapping) == len(token_ids)
 
             # TODO: have a pre-allocated buffer to hold the slot_mappings
-            slot_mapping = slot_mapping.cuda()
+            # Move to appropriate device
+
+            if hasattr(self, 'device_type') and self.device_type == "hpu":
+
+                slot_mapping = slot_mapping.to('hpu', non_blocking=True)
+
+            elif hasattr(self, 'device_type') and self.device_type == "cuda":
+
+                slot_mapping = slot_mapping.cuda()
+
+            else:
+
+                slot_mapping = slot_mapping.to(
+
+                    self.lmcache_engine.gpu_connector.device, non_blocking=True)
 
             skip_leading_tokens = save_spec.skip_leading_tokens
             if self.kv_role == "kv_producer":
@@ -1111,6 +1192,10 @@ class LMCacheConnectorV1Impl:
             if request.disagg_spec:
                 request.disagg_spec.num_transferred_tokens = len(token_ids)
 
+        # Final HPU synchronization after all saves complete
+        if hasattr(self, 'device_type') and self.device_type == "hpu" and htcore is not None:
+            htcore.mark_step()
+
     @_lmcache_nvtx_annotate
     def get_finished(
         self, finished_req_ids: set[str]
-- 
2.51.0

