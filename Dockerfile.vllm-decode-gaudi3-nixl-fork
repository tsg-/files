# SPDX-License-Identifier: Apache-2.0
# syntax=docker/dockerfile:1.4

# Dockerfile.vllm-decode-gaudi3-nixl-fork
# vLLM Decode Node for Intel Gaudi3 with NIXL and LMCache (using local forks)
# Starts from nixlbench base image
#
# Build from directory containing vllm-fork/ and LMCache-fork/:
#   docker build --target nixlbench -f Dockerfile.nixlbench -t nixlbench .
#   docker build -f Dockerfile.vllm-decode-gaudi3-nixl-fork -t vllm-decode-gaudi3-nixl-fork:latest .

FROM nixlbench AS vllm-decode-gaudi3-nixl-fork

ARG DEFAULT_PYTHON_VERSION="3.12"

ENV DEBIAN_FRONTEND=noninteractive
ENV TOPDIR=/workspace

# Gaudi-specific environment
ENV HABANA_VISIBLE_DEVICES=all
ENV PT_HPU_LAZY_MODE=1

# =============================================================================
# Rebuild UCX-Gaudi with Gaudi support (nixlbench has UCX without ROCm/Gaudi)
# =============================================================================
WORKDIR /workspace
RUN if [ -d /workspace/ucx-gaudi ]; then \
      cd /workspace/ucx-gaudi && \
      make clean || true && \
      ./autogen.sh && \
      ./contrib/configure-release \
        --prefix=$PREFIX \
        --without-rocm \
        --with-gaudi=/usr \
        --with-verbs \
        --with-rdmacm \
        --enable-mt \
        --disable-logging \
        --disable-debug \
        --disable-assertions \
        --disable-params-check && \
      make -j$(nproc) && make install && ldconfig; \
    else \
      echo "Warning: ucx-gaudi source not found, using existing UCX build"; \
    fi

# =============================================================================
# Rebuild NIXL to link against UCX with Gaudi support
# =============================================================================
WORKDIR /workspace/nixl
RUN rm -rf builddir && \
    uv run --active meson setup \
      --wipe \
      --prefix=$PREFIX \
      --buildtype=release \
      -Ddisable_gds_backend=true \
      -Dlibfabric_path=$PREFIX \
      -Ducx_path=$PREFIX \
      builddir .

RUN cd builddir && \
    ninja && \
    ninja install && \
    ldconfig && \
    cd .. && rm -rf builddir

# Rebuild nixlbench
WORKDIR /workspace/nixl/benchmark/nixlbench/
RUN rm -rf builddir && \
    uv run --active meson setup \
      --wipe \
      --prefix=$PREFIX \
      -Dnixl_path=$PREFIX \
      -Dcudapath_inc='' \
      -Dcudapath_lib='' \
      -Dcudapath_stub='' \
      builddir .

RUN cd builddir && \
    ninja && \
    ninja install && \
    ldconfig

# =============================================================================
# Build vLLM with Gaudi support (using local fork)
# =============================================================================
WORKDIR ${TOPDIR}

# Copy vLLM fork from host
COPY vllm-fork/ ${TOPDIR}/vllm/
WORKDIR ${TOPDIR}/vllm

# Install build requirements (excluding torch to reuse Habana's torch)
# Use BuildKit cache mount to speed up pip downloads
RUN --mount=type=cache,target=/root/.cache/pip \
    echo "=== [VLLM BUILD] Checking torch version before installing build requirements ===" && \
    pip show torch || echo "torch not installed yet" && \
    grep -v '^torch' requirements/build.txt > /tmp/build_requirements.txt && \
    echo "=== [VLLM BUILD] Installing build requirements (torch excluded) ===" && \
    pip install -v -r /tmp/build_requirements.txt && \
    rm /tmp/build_requirements.txt && \
    echo "=== [VLLM BUILD] Checking torch version after installing build requirements ===" && \
    pip show torch

# Install vLLM runtime requirements (excluding torch)
# Process hpu.txt in place to preserve relative paths like '-r common.txt'
RUN --mount=type=cache,target=/root/.cache/pip \
    echo "=== [VLLM HPU] Checking torch version before installing HPU requirements ===" && \
    pip show torch && \
    if [ -f requirements/hpu.txt ]; then \
        sed '/^torch$/d; /^torch[>=<~!]/d; /^torch /d' requirements/hpu.txt > /tmp/hpu_requirements.txt && \
        mv /tmp/hpu_requirements.txt requirements/hpu_filtered.txt && \
        echo "=== [VLLM HPU] Installing HPU requirements (torch excluded) ===" && \
        cat requirements/hpu_filtered.txt && \
        pip install -v -r requirements/hpu_filtered.txt || true; \
    fi && \
    echo "=== [VLLM HPU] Checking torch version after installing HPU requirements ===" && \
    pip show torch

# Build vLLM with Gaudi support (vllm-fork already has Gaudi support built-in)
# Note: vllm-fork is assumed to be the HPU/Gaudi-enabled version
# Use --no-deps to prevent pip from installing torch dependencies
ENV VLLM_TARGET_DEVICE=hpu
RUN --mount=type=cache,target=/root/.cache/pip \
    echo "=== [VLLM INSTALL] Checking torch version before vLLM installation ===" && \
    pip show torch && \
    echo "=== [VLLM INSTALL] Installing vLLM with --no-deps to prevent torch replacement ===" && \
    pip install -v --no-build-isolation --no-deps -e . && \
    echo "=== [VLLM INSTALL] Checking torch version after vLLM installation ===" && \
    pip show torch

# =============================================================================
# Install LMCache (using local fork with HPU support)
# =============================================================================
WORKDIR ${TOPDIR}

RUN --mount=type=cache,target=/root/.cache/pip \
    pip install boto3 botocore pandas msgspec pyzmq aiofile redis sortedcontainers

# Copy LMCache fork from host (assumes HPU support is already built-in)
COPY LMCache-fork/ ${TOPDIR}/LMCache/

WORKDIR ${TOPDIR}/LMCache

# Install build requirements (excluding torch to reuse Habana's torch)
RUN --mount=type=cache,target=/root/.cache/pip \
    echo "=== [LMCACHE BUILD] Checking torch version before installing build requirements ===" && \
    pip show torch && \
    grep -v '^torch' requirements/build.txt > /tmp/lmcache_build_requirements.txt && \
    echo "=== [LMCACHE BUILD] Installing build requirements (torch excluded) ===" && \
    pip install -v -r /tmp/lmcache_build_requirements.txt || true && \
    rm /tmp/lmcache_build_requirements.txt && \
    echo "=== [LMCACHE BUILD] Checking torch version after installing build requirements ===" && \
    pip show torch

# Install LMCache with HPU/Gaudi support
# PT_HPU_GPU_MIGRATION=1 enables GPU-to-HPU migration layer during build only
# Use --no-build-isolation to prevent pip from installing torch==2.7.1 from pyproject.toml
# NOTE: We don't set PT_HPU_GPU_MIGRATION globally to avoid runtime conflicts
RUN --mount=type=cache,target=/root/.cache/pip \
    echo "=== [LMCACHE INSTALL] Checking torch version before LMCache installation ===" && \
    pip show torch && \
    echo "=== [LMCACHE INSTALL] Installing LMCache with --no-build-isolation to preserve Habana torch ===" && \
    PT_HPU_GPU_MIGRATION=1 pip install -v --no-build-isolation -e . && \
    echo "=== [LMCACHE INSTALL] Checking torch version after LMCache installation ===" && \
    pip show torch

# Verify LMCache installation
RUN python -c "import lmcache; print('LMCache installed successfully')" 2>/dev/null || \
    echo "LMCache not available (will use vLLM prefix caching only)"

# Fix AutoConfig registration conflicts in vLLM
# Add exist_ok=True to prevent conflicts when configs are registered multiple times
RUN sed -i 's/AutoConfig.register("aimv2", AIMv2Config)/AutoConfig.register("aimv2", AIMv2Config, exist_ok=True)/' /workspace/vllm/vllm/transformers_utils/configs/ovis.py && \
    sed -i 's/AutoConfig.register("aimv2_visual_tokenizer", Aimv2VisualTokenizerConfig)/AutoConfig.register("aimv2_visual_tokenizer", Aimv2VisualTokenizerConfig, exist_ok=True)/' /workspace/vllm/vllm/transformers_utils/configs/ovis.py

# Final torch verification
RUN echo "=== [FINAL] Final torch package verification ===" && \
    pip show torch && \
    echo "=== [FINAL] Torch location and version ===" && \
    python -c "import torch; print(f'Torch version: {torch.__version__}'); print(f'Torch location: {torch.__file__}'); print(f'HPU available: {torch.hpu.is_available() if hasattr(torch, \"hpu\") else \"N/A\"}')"

# =============================================================================
# Update benchmark scripts for Gaudi3
# =============================================================================

# NIXL benchmark script for Gaudi3 (UCX with Gaudi transport)
COPY <<'NIXLBENCH_GAUDI3' $HOME/.local/bin/nixlbench-gaudi3-ucx.sh
#!/usr/bin/env bash
set -euo pipefail

export ETCD=${ETCD:-http://127.0.0.1:2379}
export START=${START:-4096}
export MAX=${MAX:-67108864}
export NITER=${NITER:-1000}
export NTH=${NTH:-4}
export WARM=${WARM:-100}

# UCX with Gaudi transport
export UCX_TLS=${UCX_TLS:-rc,gaudi,sm,self}
export UCX_NET_DEVICES=${UCX_NET_DEVICES:-mlx5_0:1}
export UCX_IB_GPU_DIRECT_RDMA=yes
export UCX_MEMTYPE_CACHE=n
export UCX_RC_TM_ENABLE=y
export UCX_LOG_LEVEL=error
export NIXL_LOG_LEVEL=${NIXL_LOG_LEVEL:-WARN}

exec "$HOME/.local/bin/nixlbench" \
  --etcd-endpoints "$ETCD" \
  --backend UCX \
  --initiator_seg_type DRAM --target_seg_type DRAM \
  --scheme pairwise --mode SG --op_type WRITE \
  --start_block_size "$START" --max_block_size "$MAX" \
  --num_iter "$NITER" --warmup_iter "$WARM" \
  --num_threads "$NTH"
NIXLBENCH_GAUDI3

RUN chmod +x $HOME/.local/bin/nixlbench-gaudi3-ucx.sh

# NIXL benchmark script for object storage testing
COPY <<'NIXLBENCH_OBJ' $HOME/.local/bin/nixlbench-obj.sh
#!/usr/bin/env bash
set -euo pipefail
export ETCD=${ETCD:-http://127.0.0.1:2379}
export MAX=${MAX:-67108864}
export NITER=${NITER:-1}
export NTH=${NTH:-4}
export BATCHSZ=${BATCHSZ:-16}
export BUFFERSZ=${BUFFERSZ:-17179869184}
export WARM=${WARM:-1}
export OBJ_BUCKET_NAME=${OBJ_BUCKET_NAME:-nixl-bucket}
export OBJ_SCHEME=${OBJ_SCHEME:-http}
export OBJ_REGION=${OBJ_REGION:-us-east-1}
export OBJ_ACCESS_KEY=${OBJ_ACCESS_KEY:-minioadmin}
export OBJ_SECRET_KEY=${OBJ_SECRET_KEY:-minioadmin}
export OBJ_ENDPOINT=${OBJ_ENDPOINT:-http://127.0.0.1:9000}
export OBJ_VIRTUAL_ADDRESSING=${OBJ_VIRTUAL_ADDRESSING:-0}
export NIXL_LOG_LEVEL=${NIXL_LOG_LEVEL:-WARN}
exec "$HOME/.local/bin/nixlbench" \
  --etcd-endpoints "$ETCD" \
  --backend OBJ \
  --obj_bucket_name "$OBJ_BUCKET_NAME" \
  --obj_scheme "$OBJ_SCHEME" \
  --obj_region "$OBJ_REGION" \
  --obj_access_key "$OBJ_ACCESS_KEY" \
  --obj_secret_key "$OBJ_SECRET_KEY" \
  --obj_endpoint_override "$OBJ_ENDPOINT" \
  --obj_use_virtual_addressing "$OBJ_VIRTUAL_ADDRESSING" \
  --num_iter "$NITER" --warmup_iter "$WARM" \
  --num_threads "$NTH" \
  --max_batch_size "$BATCHSZ" \
  --total_buffer_size "$BUFFERSZ"
NIXLBENCH_OBJ

RUN chmod +x $HOME/.local/bin/nixlbench-obj.sh

# =============================================================================
# Save Build Version Information
# =============================================================================
RUN mkdir -p /app && \
    echo "=== Build Version Information ===" > /app/versions.txt && \
    echo "BASE_IMAGE: nixlbench (Gaudi base)" >> /app/versions.txt && \
    echo "PYTHON_VERSION: ${DEFAULT_PYTHON_VERSION}" >> /app/versions.txt && \
    echo "" >> /app/versions.txt && \
    echo "=== Dependency Versions (from nixlbench base) ===" >> /app/versions.txt && \
    (fi_info --version 2>/dev/null | head -1 | sed 's/^/LIBFABRIC_VERSION: /' >> /app/versions.txt || echo "LIBFABRIC_VERSION: unknown" >> /app/versions.txt) && \
    (ucx_info -v 2>/dev/null | head -1 | sed 's/^/UCX_VERSION: /' >> /app/versions.txt || echo "UCX_VERSION: unknown" >> /app/versions.txt) && \
    echo "" >> /app/versions.txt && \
    echo "=== UCX-Gaudi Version ===" >> /app/versions.txt && \
    (if [ -d /workspace/ucx-gaudi ]; then \
        cd /workspace/ucx-gaudi && \
        echo "UCX_GAUDI_REPO: local/ucx-gaudi" >> /app/versions.txt && \
        echo "UCX_GAUDI_COMMIT: $(git rev-parse HEAD 2>/dev/null || echo 'unknown')" >> /app/versions.txt; \
     else \
        echo "UCX_GAUDI: not rebuilt, using base UCX" >> /app/versions.txt; \
     fi) && \
    echo "" >> /app/versions.txt && \
    echo "=== NIXL Version ===" >> /app/versions.txt && \
    (cd /workspace/nixl && echo "NIXL_REPO: $(git remote get-url origin 2>/dev/null || echo 'unknown')" >> /app/versions.txt) && \
    (cd /workspace/nixl && echo "NIXL_BRANCH: $(git rev-parse --abbrev-ref HEAD 2>/dev/null || echo 'unknown')" >> /app/versions.txt) && \
    (cd /workspace/nixl && echo "NIXL_COMMIT: $(git rev-parse HEAD 2>/dev/null || echo 'unknown')" >> /app/versions.txt) && \
    echo "" >> /app/versions.txt && \
    echo "=== vLLM Version (fork) ===" >> /app/versions.txt && \
    (cd /workspace/vllm && echo "VLLM_REPO: $(git remote get-url origin 2>/dev/null || echo 'local-fork')" >> /app/versions.txt) && \
    (cd /workspace/vllm && echo "VLLM_BRANCH: $(git rev-parse --abbrev-ref HEAD 2>/dev/null || echo 'unknown')" >> /app/versions.txt) && \
    (cd /workspace/vllm && echo "VLLM_COMMIT: $(git rev-parse HEAD 2>/dev/null || echo 'unknown')" >> /app/versions.txt) && \
    echo "VLLM_TARGET_DEVICE: hpu" >> /app/versions.txt && \
    echo "" >> /app/versions.txt && \
    echo "=== LMCache Version (fork) ===" >> /app/versions.txt && \
    (cd /workspace/LMCache && echo "LMCACHE_REPO: $(git remote get-url origin 2>/dev/null || echo 'local-fork')" >> /app/versions.txt) && \
    (cd /workspace/LMCache && echo "LMCACHE_BRANCH: $(git rev-parse --abbrev-ref HEAD 2>/dev/null || echo 'unknown')" >> /app/versions.txt) && \
    (cd /workspace/LMCache && echo "LMCACHE_COMMIT: $(git rev-parse HEAD 2>/dev/null || echo 'unknown')" >> /app/versions.txt) && \
    echo "" >> /app/versions.txt && \
    echo "=== PyTorch Version ===" >> /app/versions.txt && \
    (python -c "import torch; print(f'PYTORCH_VERSION: {torch.__version__}')" >> /app/versions.txt 2>/dev/null || echo "PYTORCH_VERSION: unknown" >> /app/versions.txt) && \
    (python -c "import habana_frameworks.torch as htorch; print('HABANA_PYTORCH: available')" >> /app/versions.txt 2>/dev/null || echo "HABANA_PYTORCH: unknown" >> /app/versions.txt) && \
    echo "" >> /app/versions.txt && \
    echo "=== Gaudi Runtime ===" >> /app/versions.txt && \
    (hl-smi --version 2>/dev/null >> /app/versions.txt || echo "HL-SMI: unknown" >> /app/versions.txt) && \
    echo "" >> /app/versions.txt && \
    echo "=== Build Timestamp ===" >> /app/versions.txt && \
    date -u >> /app/versions.txt && \
    echo "" >> /app/versions.txt && \
    cat /app/versions.txt

# =============================================================================
# Entrypoint and Health Check
# =============================================================================

COPY <<'ENTRYPOINT' /entrypoint-gaudi3-nixl.sh
#!/bin/bash
set -e

echo "=== vLLM Decode Node (Intel Gaudi3) with NIXL - Fork Version ==="
echo "Habana Runtime: $(hl-smi --version 2>/dev/null || echo 'unknown')"
echo "Visible Devices: ${HABANA_VISIBLE_DEVICES:-all}"

# Verify Gaudi devices
hl-smi -L 2>/dev/null || echo "Warning: hl-smi not available"

# Verify UCX transports
echo "UCX Transports:"
ucx_info -d | grep -E "(Transport|Device)" | head -20 || echo "Warning: ucx_info not available"

# Verify libraries
echo "Checking libraries..."
ldconfig
ldd $(which python) | grep -E "(libfabric|ucx)" || echo "Warning: libfabric/ucx not linked"

# Verify NIXL
nixlbench --help 2>&1 | head -5 || echo "Warning: nixlbench not available"

exec "$@"
ENTRYPOINT

RUN chmod +x /entrypoint-gaudi3-nixl.sh

HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:8001/health || exit 1

WORKDIR /workspace

ENTRYPOINT ["/entrypoint-gaudi3-nixl.sh"]
CMD ["python", "-m", "vllm.entrypoints.openai.api_server", "--help"]
