# SPDX-License-Identifier: Apache-2.0
# syntax=docker/dockerfile:1.4

# Dockerfile.vllm-decode-gaudi3-nixl
# vLLM Decode Node for Intel Gaudi3 with NIXL, UCX-Gaudi, and LMCache
# Starts from nixlbench base image
#
# Build: docker build --target nixlbench -f Dockerfile.nixlbench -t nixlbench .
#        docker build -f Dockerfile.vllm-decode-gaudi3-nixl -t vllm-decode-gaudi3-nixl:latest .

FROM nixlbench AS vllm-decode-gaudi3-nixl

ARG DEFAULT_PYTHON_VERSION="3.12"

ENV DEBIAN_FRONTEND=noninteractive
ENV TOPDIR=/workspace

# Gaudi-specific environment
ENV HABANA_VISIBLE_DEVICES=all
ENV PT_HPU_LAZY_MODE=1

# =============================================================================
# Rebuild UCX-Gaudi with Gaudi support (nixlbench has UCX without ROCm/Gaudi)
# =============================================================================
WORKDIR /workspace
RUN if [ -d /workspace/ucx-gaudi ]; then \
      cd /workspace/ucx-gaudi && \
      make clean || true && \
      ./autogen.sh && \
      ./contrib/configure-release \
        --prefix=$PREFIX \
        --without-rocm \
        --with-gaudi=/usr \
        --with-verbs \
        --with-rdmacm \
        --enable-mt \
        --disable-logging \
        --disable-debug \
        --disable-assertions \
        --disable-params-check && \
      make -j$(nproc) && make install && ldconfig; \
    else \
      echo "Warning: ucx-gaudi source not found, using existing UCX build"; \
    fi

# =============================================================================
# Rebuild NIXL to link against UCX with Gaudi support
# =============================================================================
WORKDIR /workspace/nixl
RUN rm -rf builddir && \
    uv run --active meson setup \
      --wipe \
      --prefix=$PREFIX \
      --buildtype=release \
      -Ddisable_gds_backend=true \
      -Dlibfabric_path=$PREFIX \
      -Ducx_path=$PREFIX \
      builddir .

RUN cd builddir && \
    ninja && \
    ninja install && \
    ldconfig && \
    cd .. && rm -rf builddir

# Rebuild nixlbench
WORKDIR /workspace/nixl/benchmark/nixlbench/
RUN rm -rf builddir && \
    uv run --active meson setup \
      --wipe \
      --prefix=$PREFIX \
      -Dnixl_path=$PREFIX \
      -Dcudapath_inc='' \
      -Dcudapath_lib='' \
      -Dcudapath_stub='' \
      builddir .

RUN cd builddir && \
    ninja && \
    ninja install && \
    ldconfig

# =============================================================================
# Build vLLM with Gaudi support
# =============================================================================
WORKDIR ${TOPDIR}

# Clone vLLM-Gaudi to get stable commit reference
RUN git clone --progress -v https://github.com/vllm-project/vllm-gaudi ${TOPDIR}/vllm-gaudi

# Clone base vLLM
RUN git clone --progress -v https://github.com/vllm-project/vllm ${TOPDIR}/vllm
WORKDIR ${TOPDIR}/vllm

# Checkout stable commit for vLLM-Gaudi compatibility
RUN export VLLM_COMMIT_HASH=$(cd ${TOPDIR}/vllm-gaudi && git show "origin/vllm/last-good-commit-for-vllm-gaudi:VLLM_STABLE_COMMIT" 2>/dev/null || echo "v0.10.0") \
    && git checkout $VLLM_COMMIT_HASH || git checkout v0.10.0

# Install build requirements (excluding torch to reuse Habana's torch)
RUN grep -v '^torch' requirements/build.txt > /tmp/build_requirements.txt && \
    pip install -r /tmp/build_requirements.txt && \
    rm /tmp/build_requirements.txt

# Build vLLM with empty device as base
ENV VLLM_TARGET_DEVICE=empty
RUN pip install --no-build-isolation -e .

# Build vLLM-Gaudi extension
WORKDIR ${TOPDIR}/vllm-gaudi
RUN pip install -e .

# =============================================================================
# Install LMCache
# =============================================================================
WORKDIR ${TOPDIR}

RUN pip install boto3 botocore pandas msgspec pyzmq aiofile redis sortedcontainers

# Clone LMCache
RUN git clone --depth=1 -b wip-nixl-obj https://github.com/mmgaggle/LMCache.git ${TOPDIR}/LMCache

WORKDIR ${TOPDIR}/LMCache

# Install build requirements
RUN pip install -r requirements/build.txt || true

# Patch setup.py to skip CUDA extensions for Gaudi-only environment
# This must be done BEFORE pip install attempts to parse setup.py
RUN python3 << 'PATCH_SETUP'
import re

print("Reading setup.py...")
with open('setup.py', 'r') as f:
    content = f.read()

original = content
patches = 0

# Patch 1: Replace the entire cuda_extension function to return empty list
pattern1 = r'def cuda_extension\([^)]*\):.*?(?=\ndef [a-z_]+|class |if __name__|$)'
if re.search(pattern1, content, re.DOTALL):
    content = re.sub(pattern1, 'def cuda_extension(*args, **kwargs):\n    """Disabled for non-CUDA environments"""\n    return []\n\n', content, flags=re.DOTALL)
    patches += 1
    print('✓ Patch 1: Replaced cuda_extension() function')

# Patch 2: Replace cuda_extensions = cuda_extension(...) calls
pattern2 = r'cuda_extensions\s*=\s*cuda_extension\([^)]*\)'
if re.search(pattern2, content):
    content = re.sub(pattern2, 'cuda_extensions = []  # Disabled for Gaudi', content)
    patches += 1
    print('✓ Patch 2: Set cuda_extensions = []')

# Patch 3: Replace ext_modules = cuda_extensions
pattern3 = r'ext_modules\s*=\s*cuda_extensions'
if re.search(pattern3, content):
    content = re.sub(pattern3, 'ext_modules = []  # Disabled for Gaudi', content)
    patches += 1
    print('✓ Patch 3: Set ext_modules = []')

# Patch 4: Remove "Building CUDA extensions" print
content = re.sub(r'print\(["\']Building CUDA extensions["\']\)', 'print("Skipping CUDA extensions for Gaudi")', content)

if patches == 0:
    print('WARNING: No patches applied! Pattern not found in setup.py')
    print('First 50 lines of setup.py:')
    print('\n'.join(content.split('\n')[:50]))
else:
    with open('setup.py', 'w') as f:
        f.write(content)
    print(f'✓ Successfully applied {patches} patches to setup.py')
PATCH_SETUP

# Set CUDA_HOME to a dummy path to avoid errors (Gaudi doesn't have CUDA)
ENV CUDA_HOME=/usr
ENV LMCACHE_SKIP_CUDA_BUILD=1

# Install LMCache without CUDA extensions
RUN pip install -e . --no-build-isolation --no-deps || \
    (echo "ERROR: LMCache installation failed after patching. Check setup.py patch." && \
     echo "Continuing without LMCache..." && true)

# Verify LMCache installation
RUN python -c "import lmcache; print(f'LMCache: {lmcache.__version__}')" 2>/dev/null || \
    echo "LMCache not available (will use vLLM prefix caching only)"

# Patch LMCache to fix HPU/Gaudi device detection issues
RUN python3 << 'PATCH_LMCACHE'
import re
import os

adapter_file = '/usr/local/lib/python3.12/dist-packages/lmcache/integration/vllm/vllm_v1_adapter.py'

if not os.path.exists(adapter_file):
    print('⚠ LMCache adapter not found, skipping patches')
    exit(0)

with open(adapter_file, 'r') as f:
    content = f.read()

patches_applied = 0

# Patch 1: Fix ZeroDivisionError when num_gpus=0
pattern1 = r'(\s+)local_rank = parallel_config\.rank % num_gpus'
replacement1 = r'\1# Fixed for HPU/Gaudi: handle num_gpus=0\n\1local_rank = parallel_config.rank % num_gpus if num_gpus > 0 else parallel_config.rank'
if re.search(pattern1, content):
    content = re.sub(pattern1, replacement1, content)
    patches_applied += 1
    print('✓ Patch 1: Fixed ZeroDivisionError (num_gpus=0)')

# Patch 2: Fix torch.cuda.set_device() call that doesn't exist in Habana PyTorch
pattern2 = r'(\s+)torch\.cuda\.set_device\(local_rank\)'
replacement2 = r'\1# Fixed for HPU/Gaudi: torch.cuda.set_device not available\n\1# HPU device selection is handled by habana_frameworks\n\1pass  # torch.cuda.set_device(local_rank)'
if re.search(pattern2, content):
    content = re.sub(pattern2, replacement2, content)
    patches_applied += 1
    print('✓ Patch 2: Fixed torch.cuda.set_device() for HPU')

if patches_applied > 0:
    with open(adapter_file, 'w') as f:
        f.write(content)
    print(f'✓ Applied {patches_applied} patch(es) to LMCache for HPU/Gaudi support')
else:
    print('⚠ No patches applied (patterns not found or already patched)')

PATCH_LMCACHE

# =============================================================================
# Update benchmark scripts for Gaudi3
# =============================================================================

# NIXL benchmark script for Gaudi3 (UCX with Gaudi transport)
COPY <<'NIXLBENCH_GAUDI3' $HOME/.local/bin/nixlbench-gaudi3-ucx.sh
#!/usr/bin/env bash
set -euo pipefail

export ETCD=${ETCD:-http://127.0.0.1:2379}
export START=${START:-4096}
export MAX=${MAX:-67108864}
export NITER=${NITER:-1000}
export NTH=${NTH:-4}
export WARM=${WARM:-100}

# UCX with Gaudi transport
export UCX_TLS=${UCX_TLS:-rc,gaudi,sm,self}
export UCX_NET_DEVICES=${UCX_NET_DEVICES:-mlx5_0:1}
export UCX_IB_GPU_DIRECT_RDMA=yes
export UCX_MEMTYPE_CACHE=n
export UCX_RC_TM_ENABLE=y
export UCX_LOG_LEVEL=error
export NIXL_LOG_LEVEL=${NIXL_LOG_LEVEL:-WARN}

exec "$HOME/.local/bin/nixlbench" \
  --etcd-endpoints "$ETCD" \
  --backend UCX \
  --initiator_seg_type DRAM --target_seg_type DRAM \
  --scheme pairwise --mode SG --op_type WRITE \
  --start_block_size "$START" --max_block_size "$MAX" \
  --num_iter "$NITER" --warmup_iter "$WARM" \
  --num_threads "$NTH"
NIXLBENCH_GAUDI3

RUN chmod +x $HOME/.local/bin/nixlbench-gaudi3-ucx.sh

# NIXL benchmark script for object storage testing
COPY <<'NIXLBENCH_OBJ' $HOME/.local/bin/nixlbench-obj.sh
#!/usr/bin/env bash
set -euo pipefail
export ETCD=${ETCD:-http://127.0.0.1:2379}
export MAX=${MAX:-67108864}
export NITER=${NITER:-1}
export NTH=${NTH:-4}
export BATCHSZ=${BATCHSZ:-16}
export BUFFERSZ=${BUFFERSZ:-17179869184}
export WARM=${WARM:-1}
export OBJ_BUCKET_NAME=${OBJ_BUCKET_NAME:-nixl-bucket}
export OBJ_SCHEME=${OBJ_SCHEME:-http}
export OBJ_REGION=${OBJ_REGION:-us-east-1}
export OBJ_ACCESS_KEY=${OBJ_ACCESS_KEY:-minioadmin}
export OBJ_SECRET_KEY=${OBJ_SECRET_KEY:-minioadmin}
export OBJ_ENDPOINT=${OBJ_ENDPOINT:-http://127.0.0.1:9000}
export OBJ_VIRTUAL_ADDRESSING=${OBJ_VIRTUAL_ADDRESSING:-0}
export NIXL_LOG_LEVEL=${NIXL_LOG_LEVEL:-WARN}
exec "$HOME/.local/bin/nixlbench" \
  --etcd-endpoints "$ETCD" \
  --backend OBJ \
  --obj_bucket_name "$OBJ_BUCKET_NAME" \
  --obj_scheme "$OBJ_SCHEME" \
  --obj_region "$OBJ_REGION" \
  --obj_access_key "$OBJ_ACCESS_KEY" \
  --obj_secret_key "$OBJ_SECRET_KEY" \
  --obj_endpoint_override "$OBJ_ENDPOINT" \
  --obj_use_virtual_addressing "$OBJ_VIRTUAL_ADDRESSING" \
  --num_iter "$NITER" --warmup_iter "$WARM" \
  --num_threads "$NTH" \
  --max_batch_size "$BATCHSZ" \
  --total_buffer_size "$BUFFERSZ"
NIXLBENCH_OBJ

RUN chmod +x $HOME/.local/bin/nixlbench-obj.sh

# =============================================================================
# Entrypoint and Health Check
# =============================================================================

COPY <<'ENTRYPOINT' /entrypoint-gaudi3-nixl.sh
#!/bin/bash
set -e

echo "=== vLLM Decode Node (Intel Gaudi3) with NIXL ==="
echo "Habana Runtime: $(hl-smi --version 2>/dev/null || echo 'unknown')"
echo "Visible Devices: ${HABANA_VISIBLE_DEVICES:-all}"

# Verify Gaudi devices
hl-smi -L 2>/dev/null || echo "Warning: hl-smi not available"

# Verify UCX transports
echo "UCX Transports:"
ucx_info -d | grep -E "(Transport|Device)" | head -20 || echo "Warning: ucx_info not available"

# Verify libraries
echo "Checking libraries..."
ldconfig
ldd $(which python) | grep -E "(libfabric|ucx)" || echo "Warning: libfabric/ucx not linked"

# Verify NIXL
nixlbench --help 2>&1 | head -5 || echo "Warning: nixlbench not available"

exec "$@"
ENTRYPOINT

RUN chmod +x /entrypoint-gaudi3-nixl.sh

HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:8001/health || exit 1

WORKDIR /workspace

ENTRYPOINT ["/entrypoint-gaudi3-nixl.sh"]
CMD ["python", "-m", "vllm.entrypoints.openai.api_server", "--help"]
